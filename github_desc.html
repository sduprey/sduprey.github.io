<html>
	<head>
		<title></title>
	</head>
	<body>
		<h2>
			SEMANTIC WEB/ SEO</h2>
		<h3>
			<a href="https://github.com/sduprey/SEO_REPO">https://github.com/sduprey/SEO_REPO</a></h3>
		<p>
			Keywords enrichment by autocompletion (AWS, PM, RDC, CDS, ...), google suggestion scraping</p>
		<p>
			Heavy multithreaded semantic corpus crawler</p>
		<p>
			Similarity metrics implementing TF/IDF, duplicate content detection</p>
		<p>
			SERP inspector with multiple IPs and squid proxy</p>
		<p>
			Emulating geolocalisation with uule parameter</p>
		<p>
			Sitemap/List ultra fast crawler GWT API connection and data auto-refresh</p>
		<p>
			In-house Nutch plugin to advanced parsing into Hadoop</p>
		<h3>
			<a href="https://github.com/sduprey/TWEETTING_ROBOTS">https://github.com/sduprey/TWEETTING_ROBOTS</a></h3>
		<p>
			Woouuou I feel like Mary Shelley giving birth to Frankenstein : an intelligent tweeting bot chiming in wherever it finds a past quote to apply to current news</p>
		<p>
			Retweeting witty quotes to news feed @wiseman_quoting&nbsp;</p>
		<h3>
			<a href="https://github.com/sduprey/SEO_MATLAB_RD">https://github.com/sduprey/SEO_MATLAB_RD</a></h3>
		<p>
			This projects aims at building different algorithms and assess their efficience</p>
		<p>
			when it comes to assess the toxicity of a link or forecast URL SERP evolution.</p>
		<p>
			This effort is directed to avoid getting penalyzed from google filters like Penguin.</p>
		<p>
			We hereby also implement a classifier which is to say which kind of page we are in (product, products list, search, ...).</p>
		<p>
			We here assess different predictive algorithms to learn over previously penalized links and properly classify new unknown links. We here feature bagged and boosted trees.</p>
		<h3>
			<a href="https://github.com/sduprey/SEO_PYTHON_RD">https://github.com/sduprey/SEO_PYTHON_RD</a></h3>
		<p>
			A python URL list crawler and a python scrapy semantic corpus crawler</p>
		<h2>
			FINANCE&nbsp;</h2>
		<h3>
			<a href="http://https://github.com/sduprey/optimal_transaction_execution">&nbsp;https://github.com/sduprey/optimal_transaction_execution</a></h3>
		<p>
			This entry contains two topics The first item is entirely based on the following paper:</p>
		<p>
			http://sfb649.wiwi.hu-berlin.de/papers/pdf/SFB649DP2011-056.pdf</p>
		<p>
			It contains 2 MATLAB demonstrating script : DATA_preprocessing.m &amp; VAR_modeling_script.m DATA_preprocessing.m uses the LOBSTER framework (https://lobster.wiwi.hu-berlin.de/) to preprocess high frequency data from the NASDAQ Total View ITCH (csv files) allowing us to reconstruct exactly at each time the order book up to ten depths.</p>
		<p>
			Just look at the published script ! VAR_modeling_script.m contains the modeling of the whole order book as VEC/VAR process. It uses the great VAR/VEC Joahnsen cointegration framework.</p>
		<p>
			After calibrating your VAR model, you then assess the impact of an order using shock scenario (sensitivity analysis) to the VAR process. We deal with 3 scenarii : normal limit order, aggressive limit order &amp; normal market order). Play section by section the script (to open up figures which contain a lot of graphs). It contains a power point to help you present this complex topic.</p>
		<p>
			The second item is entirely based on the following paper : http://www.courant.nyu.edu/~almgren/papers/optliq.pdf</p>
		<p>
			It contains a mupad document : symbolic_demo.mn I did struggle to get something nice with the symbolic toolbox. I was not able to drive a continuous workflow and had to recode some equations myself. I nevertheless managed to get a closed form solution for the simplified linear cost model. It contains a MATLAB demonstrating script : working_script.m For more sophisticated cost model, there is no more closed form and we there highlighted MATLAB numerical optimization abilities (fmincon). It contains an Optimization Apps you can install. Just launch the optimization with the default parameters. And then switch the slider between volatility risk and liquidation costs to see the trading strategies evolve on the efficient frontier. It contains a power point to help you present this complex topic.</p>
		<h3>
			<a href="https://github.com/sduprey/portfolio_backtesting">https://github.com/sduprey/portfolio_backtesting</a></h3>
		<p>
			We optimize our portoflio using constrained 130/30 efficient frontier.</p>
		<p>
			The definition and backtesting of the strategy is done through a GUI</p>
		<p>
			and outputs a dynamic report to highlight the strategy (portfolio composition at each rebalancing dates, econometrics risk indicators).</p>
		<h3>
			<a href="https://github.com/sduprey/macroeconomic_forecasting">https://github.com/sduprey/macroeconomic_forecasting</a></h3>
		<p>
			We use here a linearized DSGE (dynamic stochastic general equilibrium) to model the United States economic macro-economy. We make forecast and backtest our model over 50 years</p>
		<h2>
			MATH :</h2>
		<h3>
			<a href="https://github.com/sduprey/golden_number">https://github.com/sduprey/golden_number</a></h3>
		<p>
			This repository contains a MATLAB application which will compute the Golden number decimals up to a specified precision. It uses infinite precision arithmetic and the Fibonacci sequence</p>
		<h3>
			<a href="https://github.com/sduprey/pi_searching">https://github.com/sduprey/pi_searching</a></h3>
		<p>
			This repository contains a MATLAB application which computes and searches through Pi digits a given sequence. You&#39;ll find files to compute Pi digits, to search through and also a nice GUI to input your given number sequence (maybe your birth date).</p>
	</body>
</html>
